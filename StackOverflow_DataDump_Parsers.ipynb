{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8403c10-f943-42b3-9636-ebed66e62a3d",
   "metadata": {},
   "source": [
    "<h1>Code Snippets for Parsing the Gigantic Stack Overflow Dataset (D3)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c7177-8c13-4f9f-9c5b-a4cab14e769a",
   "metadata": {},
   "source": [
    "<p><b>Step1:</b> <u>Download the data dump from the official Stack Exchange <a href='https://archive.org/download/stackexchange'>archive</a>. </u>It contains 8 XML files namely stackoverflow.com-Badges, stackoverflow.com-Comments, stackoverflow.com-PostHistory, stackoverflow.com-PostLinks, stackoverflow.com-Posts, stackoverflow.com-Tags, stackoverflow.com-Users and stackoverflow.com-Votes</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735568f9-820e-4974-98e3-bd6fd19e14d8",
   "metadata": {},
   "source": [
    "<p><b>Step 2: </b><u>Split the large sized XML files to smaller chunks</u>. Apart from stackoverflow.com-PostLinks, and stackoverflow.com-Tags, all other files are too large to process easily in one go. So the next step is to divide them into small chunks. For this purpose, <a href='https://git-scm.com/download/win'>git bash</a> can be used which provides split command to split files line by line or by size. We used the split by line option. After extracting each of the above mentioned compressed files, open git bash terminal in the respective direcotry and type the following commands respectively. </p><br>\n",
    "<ul>split -l 50000 -d -a 5 Badges.xml Badges</ul>\n",
    "<ul>split -l 50000 -d -a 5 Users.xml Users</ul>\n",
    "<ul>split -l 50000 -d -a 8 Comments.xml Comments</ul>\n",
    "<ul>split -l 50000 -d -a 5 Votes.xml Votes</ul>\n",
    "<ul>split -l 50000 -d -a 10 Posts.xml Posts</ul>\n",
    "<ul>split -l 50000 -d -a 10 PostHistory.xml PostHistory</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63f087-644f-4a9f-9734-026e079fe46e",
   "metadata": {},
   "source": [
    "<p><b>Step 3: </b><u>Converting splitted files to proper XML files.</u> As the split operation was performed line by line, the first chunk contains XML header whereas the last chunk contains the closing tag of the respective file. The rest of the chunks lack proper XML header/footer. Make a directory and move all the respective chunks generated by running each of the commands in that directory e.g. if the original <code>Post.xml</code> was placed in <code>E:\\Dataset_2024\\stackoverflow.com-Posts</code>, after running the above command (<code>split -l 50000 -d -a 10 Posts.xml Posts</code>, the same folder will contain more than 1100 files starting from <code>Posts0000000000</code>. Excluding the original large sized Posts.xml, move the rest of the files (small chunks) to some other directory e.g. <code>PostsSplit</code>. Provide the path to this direcotry as <code>input_path</code>. Similarly, make another directory to store the resultant small sized XML files and provide its path in <code>output_path</code>. We used the following code snippet to convert these chunks into small sized XML files. Replace the XML header/footer strings in the code snippet (e.g <code> posts </code> and <code> /posts </code>) based on the file that you are dealing with e.g. posts, users, votes and so on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a2739-435d-4f47-ba7b-508cc5326797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "input_path =r'E:\\Dataset_2024\\stackoverflow.com-PostHistory\\PostHistorySplit'\n",
    "output_path=r'E:\\Dataset_2024\\stackoverflow.com-PostHistory\\PostHistorySplitXML\\\\'\n",
    "\n",
    "for filename in glob.glob(os.path.join(input_path, '*')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf8') as f: \n",
    "        print(\"Reading file {}\".format(filename))\n",
    "        Lines = f.readlines()\n",
    "        xml_lines=[]\n",
    "        xml_lines.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\n<posthistory>\")\n",
    "        xml_lines.extend(Lines)\n",
    "        xml_lines.append(\"</posthistory>\")\n",
    "\n",
    "        head, tail = os.path.split(filename)\n",
    "\n",
    "        outputfile = output_path+tail+'.xml'      \n",
    "        print(\"Writing file to {}\".format(outputfile))\n",
    "        with open(outputfile, \"w\",encoding='utf8') as output:\n",
    "            output.write(''.join (xml_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8820b-52f7-49f7-a4e6-047f470fc404",
   "metadata": {},
   "source": [
    "<p><b>Step4: </b><u>Manually correcting the first and last XML file</u>. The above code snippet does not consider any special cases. Hence, after executing the above code snippet the first file will contain the header twice and the last file will contain the closing tag twice. We need to remove the redundant tags from the first and last file manually to make them valid.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae8db3-9422-44f3-8689-ddb65af753d3",
   "metadata": {},
   "source": [
    "<p><b>Step 5: </b><u>Saving XML based data in database for further querying.</u> We used MS SQL Server Developer Edition to store the data as the Express Edition restricts the database size to 10GB at maximum. However, in our case that dataset was much larger than that. It is worth mentioning here that the Developer Edition cannot be used in production environment. We further used SQL Server Management Studio (SSMS) to easily create and manipulate databases using a GUI. Both <a href='https://www.microsoft.com/en-us/sql-server/sql-server-downloads?msockid=236f7f3cd0c269f836b36be8d4c26fdb'>SQL Server </a> and <a href='https://learn.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver16'> SSMS </a> are available for use freely by Microsoft. The default location used by SQL Server to store database files on a Windows based system is <code>C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\DATA\\</code>. Using <code>New Database</code> wizard in SSMS, create new database and change the default location for storing the dataabase files to some other directory as your <code>C:</code> drive may have limited space. Instead of storing all data in a single database, we created separate databases corresponding to each of the above mentioned XML files (i.e. <code> StackOverflowBadgesDb_April2024, StackOverflowCommentsDb_April2024, StackOverflowPostHistoryDb_April2024, StackOverflowPostLinksDb_April2024, StackOverflowPostsDb_April2024, StackOverflowTagsDb_April2024, StackOverflowUsersDb_April2024, StackOverflowVotesDb_April2024 </code> respectively.)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc395bfc-8bf4-49e2-8351-9279649dc27a",
   "metadata": {},
   "source": [
    "<p><b>Step 6: </b><u>Creating Tables for Storing Data</u>. After creating the respective databases, the next step is to create tables in which the actual data will be stored from the XML files. We used the following SQL queries to create corresponding tables. </p>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83e4f087-6348-4a29-8ff1-a70b8b81c95d",
   "metadata": {},
   "source": [
    "\n",
    "use  StackOverflowBadgesDb_April2024\n",
    "create table StackOverflowBadges (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tUserId int, \n",
    "\t\t\t\tName nvarchar(50),\n",
    "\t\t\t\tDate datetime,\n",
    "\t\t\t\tClass tinyint, \n",
    "\t\t\t\tTagBased bit)\n",
    "\n",
    "use  StackOverflowCommentsDb_April2024\n",
    "create table StackOverflowComments (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostId int,\n",
    "\t\t\t\tScore int,\n",
    "\t\t\t\tText nvarchar(600),\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tUserDisplayName nvarchar(40),\n",
    "\t\t\t\tUserId int,\n",
    "\t\t\t\tContentLicense varchar(12))\n",
    "\n",
    "use  StackOverflowPostHistoryDb_April2024\n",
    "create table StackOverflowPostHistory (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostHistoryTypeId tinyint,\n",
    "\t\t\t\tPostId int,\n",
    "\t\t\t\tRevisionGUID uniqueidentifier,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tUserId int, \n",
    "\t\t\t\tUserDisplayName nvarchar(400),\n",
    "\t\t\t\tComment nvarchar(400),\n",
    "\t\t\t\tText nvarchar(max),\n",
    "\t\t\t\tContentLicense varchar(12))\n",
    "\n",
    "use  StackOverflowPostLinksDb_April2024\n",
    "create table StackOverflowPostLinks (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tPostId int, \n",
    "\t\t\t\tRelatedPostId int,\n",
    "\t\t\t\tLinkTypeId tinyint)\n",
    "\n",
    "use  StackOverflowPostsDb_April2024\n",
    "create table StackOverflowPosts (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostTypeId tinyint,\n",
    "\t\t\t\tAcceptedAnswerId int,\n",
    "\t\t\t\tParentId int,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tDeletionDate datetime, \n",
    "\t\t\t\tScore int,\n",
    "\t\t\t\tViewCount int,\n",
    "\t\t\t\tBody nvarchar(max),\n",
    "\t\t\t\tOwnerUserId int,\n",
    "\t\t\t\tOwnerDisplayName nvarchar(40),\n",
    "\t\t\t\tLastEditorUserId int,\n",
    "\t\t\t\tLastEditorDisplayName nvarchar(40), \n",
    "\t\t\t\tLastEditDate datetime,\n",
    "\t\t\t\tLastActivityDate datetime,\n",
    "\t\t\t\tTitle nvarchar(250),\n",
    "\t\t\t\tTags nvarchar(250),\n",
    "\t\t\t\tAnswerCount int,\n",
    "\t\t\t\tCommentCount int, \n",
    "\t\t\t\tFavoriteCount int, \n",
    "\t\t\t\tClosedDate datetime,\n",
    "\t\t\t\tCommunityOwnedDate datetime,\n",
    "\t\t\t\tContentLicense varchar(12)\n",
    "\t\t\t\t)\n",
    "\n",
    "use  StackOverflowTagsDb_April2024\n",
    "create table StackOverflowTags (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tTagName nvarchar(35),\n",
    "\t\t\t\tTagCount int, \n",
    "\t\t\t\tExcerptPostId int,\n",
    "\t\t\t\tWikiPostId int)\n",
    "\n",
    "use  StackOverflowUsersDb_April2024\n",
    "create table StackOverflowUsers (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tReputation int, \n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tDisplayName nvarchar(40),\n",
    "\t\t\t\tLastAccessDate datetime,\n",
    "\t\t\t\tWebsiteUrl nvarchar(200), \n",
    "\t\t\t\tLocation nvarchar(100), \n",
    "\t\t\t\tAboutMe nvarchar(max),\n",
    "\t\t\t\tViews int, \n",
    "\t\t\t\tUpvotes int, \n",
    "\t\t\t\tDownVotes int, \n",
    "\t\t\t\tProfileImageUrl nvarchar(200), \n",
    "\t\t\t\tAccountId int)\n",
    "\n",
    "use  StackOverflowVotesDb_April2024\n",
    "create table StackOverflowVotes (AutoIdPK int Identity(1,1),\n",
    "\t\t\t\tId int, \n",
    "\t\t\t\tPostId int, \n",
    "\t\t\t\tVoteTypeId tinyint,\n",
    "\t\t\t\tUserId int,\n",
    "\t\t\t\tCreationDate datetime,\n",
    "\t\t\t\tBountyAmount int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe02e8-00b7-4a53-b558-5ba7f633666b",
   "metadata": {},
   "source": [
    "<p><b>Step 7: </b><u>Reading XML files and inserting records in respective database tables</u>. Now we need to read the small sized XML files one by one and store the data in the respective tables created above. For this purpose, the code snippet used for parsing each of the 8 XML file types namely Badges, Comments, PostHistory, PostLinks, Posts, Tags, Users, and Votes is given below. Before using these snippets, replace the server and database names and the path of directory containing splitted XML files. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4558b-c710-46de-8505-19668ea03c9a",
   "metadata": {},
   "source": [
    "<h2>Badges Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac48e0f-39f4-4170-8a05-9f112a27e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowBadgesDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "badges_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Badges\\\\BadgesSplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(badges_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    \n",
    "    badges_df = pd.read_xml(filename)\n",
    "    badges_df = badges_df.fillna(0)\n",
    "\n",
    "    print (badges_df.head())\n",
    "    print('*************')\n",
    " \n",
    "    query = \"insert into StackOverflowBadges (Id, UserId, Name, Date, Class,TagBased) values (?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, badges_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98bc0-a72b-4933-b5a2-862f62240504",
   "metadata": {},
   "source": [
    "<h2>Comments Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcd9e4-0649-43b2-b8de-19a7fc6e79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowCommentsDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "comments_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Comments\\\\CommentsSplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(comments_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    comments_df = pd.read_xml(filename)\n",
    "    comments_df = comments_df.fillna(0)\n",
    "\n",
    "    print (comments_df.head())\n",
    "    print('*************')\n",
    "    columnnames= list(comments_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    query=\"insert into StackOverflowComments (\"+columns+\") values (?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, comments_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318dd54-abd0-4591-8eb2-9c6820a2b214",
   "metadata": {},
   "source": [
    "<h2>PostHistory Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44966145-7ad9-431b-b914-1b481fd96a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowPostHistoryDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "post_history_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-PostHistory\\\\PostHistorySplitXML\\\\'\n",
    "\n",
    "for filename in glob.glob(os.path.join(post_history_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    PostHistory_df = pd.read_xml(filename)\n",
    "    \n",
    "    PostHistory_df =  PostHistory_df.fillna(value={'Id':0,'PostHistoryTypeId':0,'PostId':0,'RevisionGUID':0,'CreationDate':0,'UserId':0,'Text':'','ContentLicense':'','UserDisplayName':'', 'Comment':''})\n",
    "    print (PostHistory_df.head())\n",
    "    print('*************')\n",
    "    columnnames= list(PostHistory_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    query=\"insert into StackOverflowPostHistory (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?)\"\n",
    "    \n",
    "    # query = \"insert into StackOverflowPostHistory(Id,PostHistoryTypeId,PostId,RevisionGUID,CreationDate,UserId,Text,ContentLicense,Comment,UserDisplayName) values (?,?,?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, PostHistory_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449b1a3-f743-471c-962d-0424ce8e1513",
   "metadata": {},
   "source": [
    "<h2>PostLinks Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b43d60-57f5-4825-8678-2b545f10556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowPostLinksDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "PostLinks_xml_path ='E:\\Dataset_2024\\stackoverflow.com-PostLinks\\PostLinks.xml'\n",
    "PostLinks_df = pd.read_xml(PostLinks_xml_path)\n",
    "PostLinks_df = PostLinks_df.fillna(0)\n",
    "\n",
    "print (PostLinks_df.head())\n",
    "print('*************')\n",
    " \n",
    "query = \"insert into StackOverflowPostLinks (Id,CreationDate,PostId,RelatedPostId,LinkTypeId) values (?,?,?,?,?)\"\n",
    "\n",
    "cursor.executemany(query, PostLinks_df.values.tolist())\n",
    "cursor.commit()\n",
    "print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38fa4-b620-421d-853e-0cab097957d3",
   "metadata": {},
   "source": [
    "<h2>Posts Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138f4f4-bfd0-47c2-841f-427f4422bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowPostsDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "posts_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Posts\\\\PostsSplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(posts_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    posts_df = pd.read_xml(filename)\n",
    "    print(posts_df.head())\n",
    "    posts_df =  posts_df.fillna(value={'Id':0,'PostTypeId':0,'AcceptedAnswerId':0,'CreationDate':'','Score':0,'ViewCount':0,'OwnerUserId':0, 'LastEditorUserId':0, 'LastEditDate':0, 'LastActivityDate':0,'AnswerCount':0,'CommentCount':0, 'ParentId':0, 'CommunityOwnedDate':0 ,  'ClosedDate':0,\n",
    "                            'FavoriteCount':0})\n",
    "    posts_df['Id']=posts_df['Id'].astype(int)\n",
    "    posts_df['PostTypeId']=posts_df['PostTypeId'].astype(int)\n",
    "    posts_df['AcceptedAnswerId']=posts_df['AcceptedAnswerId'].astype(int)\n",
    "    posts_df['CreationDate']= pd.to_datetime(posts_df['CreationDate'], format=\"mixed\")\n",
    "    posts_df['Score']=posts_df['Score'].astype(int)\n",
    "    posts_df['ViewCount']=posts_df['ViewCount'].astype(int)\n",
    "    posts_df['OwnerUserId']=posts_df['OwnerUserId'].astype(int)\n",
    "    posts_df['LastEditorUserId']=posts_df['LastEditorUserId'].astype(int)\n",
    "    posts_df['LastEditDate']= pd.to_datetime(posts_df['LastEditDate'], format=\"mixed\")\n",
    "    posts_df['LastActivityDate']= pd.to_datetime(posts_df['LastActivityDate'], format=\"mixed\")\n",
    "    posts_df['AnswerCount']=posts_df['AnswerCount'].astype(int)\n",
    "    posts_df['CommentCount']=posts_df['CommentCount'].astype(int)\n",
    "    posts_df['ParentId']=posts_df['ParentId'].astype(int)\n",
    "    posts_df['CommunityOwnedDate']= pd.to_datetime(posts_df['CommunityOwnedDate'], format=\"mixed\")\n",
    "    posts_df['ClosedDate']= pd.to_datetime(posts_df['ClosedDate'], format=\"mixed\")\n",
    "    columnnames= list(posts_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    if 'FavoriteCount' in posts_df.columns:\n",
    "        posts_df['FavoriteCount']=posts_df['FavoriteCount'].astype(int)\n",
    "        \n",
    "        query=\"insert into StackOverflowPosts (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "    else:\n",
    "        query=\"insert into StackOverflowPosts (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "    \n",
    "    #     print(posts_df.columns)\n",
    "\n",
    "    posts_df = posts_df.fillna('')\n",
    "    \n",
    "    #     query = \"insert into StackOverflowPosts (Id, PostTypeId, CreationDate, Score, ViewCount, Body,OwnerDisplayName, LastActivityDate, Title, Tags, AnswerCount,CommentCount, FavoriteCount, ContentLicense, ParentId,OwnerUserId, LastEditorUserId, LastEditorDisplayName,LastEditDate, AcceptedAnswerId, CommunityOwnedDate, ClosedDate, Tag0,Tag1,Tag2, Tag3, Tag4 ) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "#     query = \"insert into StackOverflowPosts (Id, PostTypeId, AcceptedAnswerId, CreationDate, Score,ViewCount, Body, OwnerUserId, LastEditorUserId, LastEditDate, LastActivityDate, Title, Tags, AnswerCount, CommentCount,ContentLicense, ParentId,CommunityOwnedDate, ClosedDate, OwnerDisplayName, LastEditorDisplayName, FavoriteCount, Tag0,Tag1,Tag2, Tag3, Tag4,VaderRawPositiveScore, VaderRawNegativeScore, VaderRawNeutralScore, VaderRawCompoundScore,VaderRawCompoundSentiment ) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, posts_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e0669-3296-40ec-b20f-2d0b9aff141e",
   "metadata": {},
   "source": [
    "<h2>Tags Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c408f-fe9a-413b-aafe-f6515d2df5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowTagsDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "row_list = []\n",
    "errorcount = 0\n",
    "tags_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Tags\\\\Tags.xml'\n",
    "tags_df = pd.read_xml(tags_xml_path)\n",
    "tags_df = tags_df.fillna('')\n",
    "\n",
    "print (tags_df.head())\n",
    "print('*************')\n",
    "\n",
    "query = \"insert into StackOverflowTags (Id, TagName, TagCount ,ExcerptPostId ,WikiPostId) values (?,?,?,?,?)\"\n",
    "\n",
    "cursor.executemany(query, tags_df.values.tolist())\n",
    "cursor.commit()\n",
    "print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fe9ae-4db3-4eb8-9c8a-8124675c0c62",
   "metadata": {},
   "source": [
    "<h2>Users Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965dd0b-2885-41b1-b751-d39c19169e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowUsersDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "users_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Users\\\\UsersSplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(users_xml_path, '*')):\n",
    "    print(filename)\n",
    "    users_df = pd.read_xml(filename)\n",
    "    users_df = users_df.fillna(0)\n",
    "    \n",
    "    print (users_df.head())\n",
    "    print(users_df.columns)\n",
    "    print('*************')\n",
    "    columnnames= list(users_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "#     print(columnnames)\n",
    "    query=\"insert into StackOverflowUsers (\"+columns+\") values (?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "#     query = \"insert into StackOverflowUsers ( Id, Reputation, CreationDate, DisplayName, LastAccessDate, Views, UpVotes, DownVotes, AccountId, WebsiteUrl, AboutMe,ProfileImageUrl, Location) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, users_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73a0ed-e822-4166-be6b-c84588c549b6",
   "metadata": {},
   "source": [
    "<h2>Votes Parser for Stack Overflow Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bfcb7-b395-4a82-81d4-99683a674a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "server = 'DESKTOP-DEK23E9'\n",
    "database = 'StackOverflowVotesDb_April2024' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "votes_xml_path ='E:\\\\Dataset_2024\\\\stackoverflow.com-Votes\\\\VotesSplitXML'\n",
    "\n",
    "for filename in glob.glob(os.path.join(votes_xml_path, '*')):\n",
    "\n",
    "    print(filename)\n",
    "    votes_df = pd.read_xml(filename)\n",
    "    votes_df = votes_df.fillna(0)\n",
    "\n",
    "    print (votes_df.head())\n",
    "    print('*************')\n",
    "    columnnames= list(votes_df.columns.values)\n",
    "    columns=','.join(columnnames)\n",
    "    query=\"insert into StackOverflowVotes (\"+columns+\") values (?,?,?,?,?,?)\"\n",
    "    # query = \"insert into StackOverflowVotes (Id, PostId,VoteTypeId,CreationDate) values (?,?,?,?)\"\n",
    "    # query = \"insert into StackOverflowVotes (Id, PostId,VoteTypeId,CreationDate,BountyAmount) values (?,?,?,?,?)\"\n",
    "#     query = \"insert into StackOverflowVotes (Id, PostId,VoteTypeId,CreationDate,UserId,BountyAmount) values (?,?,?,?,?,?)\"\n",
    "\n",
    "    cursor.executemany(query, votes_df.values.tolist())\n",
    "    cursor.commit()\n",
    "    print(\"Added in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4456d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
